@article{10.1093/bib/bbz075,
    author = {Ison, Jon and Ménager, Hervé and Brancotte, Bryan and Jaaniso, Erik and Salumets, Ahto and Raček, Tomáš and Lamprecht, Anna-Lena and Palmblad, Magnus and Kalaš, Matúš and Chmura, Piotr and Hancock, John M and Schwämmle, Veit and Ienasescu, Hans-Ioan},
    title = "{Community curation of bioinformatics software and data resources}",
    journal = {Briefings in Bioinformatics},
    year = {2019},
    month = {10},
    abstract = "{The corpus of bioinformatics resources is huge and expanding rapidly, presenting life scientists with a growing challenge in selecting tools that fit the desired purpose. To address this, the European Infrastructure for Biological Information is supporting a systematic approach towards a comprehensive registry of tools and databases for all domains of bioinformatics, provided under a single portal (https://bio.tools). We describe here the practical means by which scientific communities, including individual developers and projects, through major service providers and research infrastructures, can describe their own bioinformatics resources and share these via bio.tools.}",
    issn = {1477-4054},
    doi = {10.1093/bib/bbz075},
    url = {https://doi.org/10.1093/bib/bbz075},
    note = {bbz075},
    eprint = {https://academic.oup.com/bib/article-pdf/doi/10.1093/bib/bbz075/30157114/bbz075.pdf},
}


@book{ghallab_automated_2016,
	address = {New York, NY, USA},
	edition = {1st},
	title = {Automated {Planning} and {Acting}},
	isbn = {978-1-107-03727-4},
	abstract = {Autonomous AI systems need complex computational techniques for planning and performing actions. Planning and acting require significant deliberation because an intelligent system must coordinate and integrate these activities in order to act effectively in the real world. This book presents a comprehensive paradigm of planning and acting using the most recent and advanced automated-planning techniques. It explains the computational deliberation capabilities that allow an actor, whether physical or virtual, to reason about its actions, choose them, organize them purposefully, and act deliberately to achieve an objective. Useful for students, practitioners, and researchers, this book covers state-of-the-art planning techniques, acting techniques, and their integration which will allow readers to design intelligent systems that are able to act effectively in the real world.},
	publisher = {Cambridge University Press},
	author = {Ghallab, Malik and Nau, Dana and Traverso, Paolo},
	year = {2016}
}

@book{gulwani_program_2017,
	series = {Foundations and {Trends} in {Programming} {Languages}},
	title = {Program {Synthesis}},
	volume = {4},
	tmp_url = {https://www.nowpublishers.com/article/Details/PGL-010},
	language = {English},
	number = {1-2},
	urldate = {2018-08-02},
	publisher = {now},
	author = {Gulwani, Sumit and Polozov, Oleksandr and Singh, Rishabh},
	month = jul,
	year = {2017},
	file = {Full Text PDF:/home/allamprecht/ownCloud/Documents/Zotero/storage/CC5YNY88/Gulwani et al. - 2017 - Program Synthesis.pdf:application/pdf;Snapshot:/home/allamprecht/ownCloud/Documents/Zotero/storage/QCK77RXV/PGL-010.html:text/html}
}

@article{atkinson_scientific_2017,
	title = {Scientific workflows: {Past}, present and future},
	volume = {75},
	issn = {0167-739X},
	shorttitle = {Scientific workflows},
	tmp_url = {http://www.sciencedirect.com/science/article/pii/S0167739X17311202},
	tmp_doi = {10.1016/j.future.2017.05.041},
	urldate = {2018-08-02},
	journal = {Future Generation Computer Systems},
	author = {Atkinson, Malcolm and Gesing, Sandra and Montagnat, Johan and Taylor, Ian},
	month = oct,
	year = {2017},
	keywords = {Optimisation, Performance, Scientific methods, Scientific workflows, Usability},
	pages = {216--227},
	file = {ScienceDirect Full Text PDF:/home/allamprecht/ownCloud/Documents/Zotero/storage/EJ6EYKQW/Atkinson et al. - 2017 - Scientific workflows Past, present and future.pdf:application/pdf;ScienceDirect Snapshot:/home/allamprecht/ownCloud/Documents/Zotero/storage/X7ESF7HW/S0167739X17311202.html:text/html}
}

@misc{ wiki:SciWFSystems,
    author = "{Wikipedia contributors}",
    title = "Scientific workflow system --- {Wikipedia}{,} The Free Encyclopedia",
    year = "2019",
    howpublished = "\url{https://en.wikipedia.org/w/index.php?title=Scientific_workflow_system&oldid=928001704}",
    note = "[Online; accessed 3-February-2020]"
  }

@misc{apache:existingWFSystems,
	title = {Existing {Workflow} systems},
	url = {https://s.apache.org/existing-workflow-systems},
	language = {en},
	urldate = {2020-02-04},
	file = {Snapshot:/home/allamprecht/ownCloud/Documents/Zotero/storage/QJNAM6YG/Existing-Workflow-systems.html:text/html}
}

@article{scheiderCCD2020,
author={Scheider, Simon and Meerlo, Rogier and Kasalica, Vedran  and Lamprecht, Anna-Lena},
title={Ontology of core concept data types for answering geo-analytical questions},
journal={JOSIS},
year=2020,
url={https://www.josis.org/index.php/josis/article/view/555},
note={In press.}
}

@article{kruiger2020,
author={Han Kruiger and Vedran Kasalica and Rogier Meerlo and Anna-Lena Lamprecht and Simon Scheider},
title={{Loose programming of GIS workflows with geo-analytical concepts}},
journal={Transactions in GIS},
year=2020,
note={Under review},
}

@InProceedings{kasalicaLamprecht2019,
author="Kasalica, Vedran
and Lamprecht, Anna-Lena",
title="Workflow Discovery Through Semantic Constraints: A Geovisualization Case Study",
booktitle="Computational Science and Its Applications -- ICCSA 2019",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="473--488",
abstract="The construction of computational pipelines, for example automated cartographic workflows for the construction of thematic maps, typically requires detailed knowledge about the available tools for the individual steps and the technicalities of their composition. It is a time-consuming process and comes with the risk of missing meaningful workflows because many possible pipelines are never taken into account. Automated workflow composition techniques can facilitate comprehensive workflow discovery based on semantic constraints: The users express their intention about the workflows by means of high-level constraints, and receive possible workflows that meet their request. The successful application of such methods essentially depends on the availability and quality of semantic domain models that describe the tools and data types in the domain. In this paper, we present an exemplary domain model for a geovisualization use case, and show how it enables the abstract specification and automated composition of a complex cartographic workflow.",
isbn="978-3-030-24302-9"
}

@article{kasalica2019,
	title = {Workflow {Discovery} with {Semantic} {Constraints}: {The} {SAT}-{Based} {Implementation} of {APE}},
	volume = {78},
	copyright = {Copyright (c) 2020 Electronic Communications of the EASST},
	issn = {1863-2122},
	shorttitle = {Workflow {Discovery} with {Semantic} {Constraints}},
	doi = {10.14279/tuj.eceasst.78.1092},
	abstract = {Science today is increasingly computational, and many researchers regularly face the need of creating purpose-specific computational pipelines for their specific data analysis problems. The manual composition and implementation of such workflows regularly costs valuable research time. Hence, many scientists wish for a system that would only require an abstract description of their intended data analysis process, and from there automatically compose and implement suitable workflows. In this paper we describe APE (the Automated Pipeline Explorer), a new implementation of a synthesis-based workflow discovery framework that aims to accomplish such automated composition. The framework captures the required technical domain knowledge in the form of tool and type taxonomies and functional tool annotations. Based on this semantic domain model, the framework allows users to specify their intents about workflows at an abstract, conceptual level in the form of natural-language templates. Internally, APE maps them to a temporal logic and translates them into a propositional logic instance of the problem that can be solved by an off-the-shelf SAT solver. From the solutions provided by the solver, APE then constructs executable workflow implementations. First applications of APE on realistic scientific workflow scenarios have shown that it is able to efficiently synthesize meaningful workflows. We use an example from the geospatial application domain as a running example in this paper.},
	language = {en},
	number = {0},
	urldate = {2020-05-17},
	journal = {Electronic Communications of the EASST},
	author = {Kasalica, Vedran and Lamprecht, Anna-Lena},
	month = may,
	year = {2020},
	note = {Number: 0}
}


@inproceedings{simon_automated_2016,
	address = {Cham},
	title = {Automated {Spatial} {Data} {Processing} and {Refining}},
	isbn = {978-3-319-51641-7},
	abstract = {This paper focuses on the definition of a method for data processing by means of automated professional map generation. For this, initially services have to be identified that represents cartographic rules and recommendations. In order to link those services with respect to their cartographic content and to control the process within a component, a set of rules has to be designed. This is explained by examples and can be used as a template pattern for other services. Individual services and modules within the process will be arranged hierarchically on the basis of the cartographic visualisation pipeline. Its consequent graphical classification is presented. The aim is to prepare the theoretical cartographic basis in a formal way, which should enable technical implementation without cartographic technical expertise.},
	booktitle = {Leveraging {Applications} of {Formal} {Methods}, {Verification}, and {Validation}},
	publisher = {Springer International Publishing},
	author = {Simon, Marion and Asche, Hartmut},
	editor = {Lamprecht, Anna-Lena},
	year = {2016},
	pages = {38--49}
}

@inproceedings{asche_process-oriented_2012,
	address = {Berlin, Heidelberg},
	title = {Process-{Oriented} {Geoinformation} {Systems} and {Applications}},
	isbn = {978-3-642-34032-1},
	abstract = {It is a well-accepted truism that the vast majority of digital data have a geographical reference. In the past decades geodata have been processed and visualised by dedicated software products of the geoinformation systems (GIS) type for a limited range of scientific and professional applications. However, For more than a deacade, however, both geodata and GIS functionalities are having an increasing, by now almost ubiquitious impact on various fields of everyday life. Against this background space-related, process-orientend software environments will play a decisive role in the development and delivery of a variety of geoinformation and geovisualisation products and services for a wide range of scientific and practical applications alike. This track aims at providing an update on current as well as emerging issues and applications in ubiquitious geoinformation and geovisualisation.},
	booktitle = {Leveraging {Applications} of {Formal} {Methods}, {Verification} and {Validation}. {Applications} and {Case} {Studies}},
	publisher = {Springer Berlin Heidelberg},
	author = {Asche, Hartmut},
	editor = {Margaria, Tiziana and Steffen, Bernhard},
	year = {2012},
	pages = {324--324}
}

@article{kuhn_core_2012,
	title = {Core concepts of spatial information for transdisciplinary research},
	volume = {26},
	tmp_doi_comment  = {10.1080/13658816.2012.722637},
	number = {12},
	journal = {International Journal of Geographical Information Science},
	author = {Kuhn, Werner},
	year = {2012},
	pages = {2267--2276}
}

@article{palmblad_automated_2018,
	title = {Automated workflow composition in mass spectrometry-based proteomics},
	abstract = {Numerous software utilities operating on mass spectrometry (MS) data are described in the literature and provide specific operations as building blocks for the assembly of on-purpose workflows. Working out which tools and combinations are applicable or optimal in practice is often hard. Thus researchers face difficulties in selecting practical and effective data analysis pipelines for a specific experimental design.We provide a toolkit to support researchers in identifying, comparing and benchmarking multiple workflows from individual bioinformatics tools. Automated workflow composition is enabled by the tools’ semantic annotation in terms of the EDAM ontology. To demonstrate the practical use of our framework, we created and evaluated a number of logically and semantically equivalent workflows for four use cases representing frequent tasks in MS-based proteomics. Indeed we found that the results computed by the workflows could vary considerably, emphasizing the benefits of a framework that facilitates their systematic exploration.The project files and workflows are available from https://github.com/bio-tools/biotoolsCompose/tree/master/Automatic-Workflow-Composition.Supplementary data are available at Bioinformatics online.},
	author = {Palmblad, Magnus and Lamprecht, Anna-Lena and Ison, Jon and Schwämmle, Veit},
	year = {2018}
}

@article{ison2013edam,
  title={EDAM: an ontology of bioinformatics operations, types of data and identifiers, topics and formats},
  author={Ison, Jon and Kala{\v{s}}, Mat{\'u}{\v{s}} and Jonassen, Inge and Bolser, Dan and Uludag, Mahmut and McWilliam, Hamish and Malone, James and Lopez, Rodrigo and Pettifer, Steve and Rice, Peter},
  journal={Bioinformatics},
  volume={29},
  number={10},
  pages={1325--1332},
  year={2013},
  publisher={Oxford University Press}
}

@book{gdal/ogr_contributors_gdal/ogr_2018,
	title = {{GDAL}/{OGR} {Geospatial} {Data} {Abstraction} software {Library}},
	url = {http://gdal.org},
	publisher = {Open Source Geospatial Foundation},
	author = {{GDAL/OGR contributors}},
	year = {2018},
	annote = {[Online; 14 Feb 2019]}
}

@book{noauthor_csiss/gmu_nodate,
	title = {{CSISS}/{GMU} {Geospatial} {Web} {Services}},
	url = {http://cube.csiss.gmu.edu/grassweb/manuals/index.html},
	annote = {[Online; 14 Feb 2019]}
}

@book{noauthor_scientific_nodate,
	title = {Scientific workflow system},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Scientific_workflow_system&oldid=877419140},
	abstract = {A scientific workflow system is a specialized form of a workflow management system designed specifically to compose and execute a series of computational or data manipulation steps, or workflow, in a scientific application.},
	language = {en},
	urldate = {2019-02-15},
	annote = {[Online; 14 Feb 2019]}
}

@article{filiot_exploiting_2013,
	title = {Exploiting structure in {LTL} synthesis},
	volume = {15},
	issn = {1433-2787},
	Url_comment = {https://tmp_doi.org/10.1007/s10009-012-0222-5},
	tmp_doi_comment = {10.1007/s10009-012-0222-5},
	abstract = {In this paper, we show how to exploit the structure of some automata-based construction to efficiently solve the LTL synthesis problem. We focus on a construction proposed in Schewe and Finkbeiner that reduces the synthesis problem to a safety game, which can then be solved by computing the solution of the classical fixpoint equation νX.Safe ∩ CPre(X), where CPre(X) are the controllable predecessors of X. We have shown in previous works that the sets computed during the fixpoint algorithm can be equipped with a partial order that allows one to represent them very compactly, by the antichain of their maximal elements. However the computation of CPre(X) cannot be done in polynomial time when X is represented by an antichain (unless P = NP). This motivates the use of SAT solvers to compute CPre(X). Also, we show that the CPre operator can be replaced by a weaker operator CPre crit where the adversary is restricted to play a subset of critical signals. We show that the fixpoints of the two operators coincide, and so, instead of applying iteratively CPre, we can apply iteratively CPre crit. In practice, this leads to important improvements on previous LTL synthesis methods. The reduction to SAT problems and the weakening of the CPre operator into CPre crit and their performance evaluations are new.},
	number = {5},
	journal = {International Journal on Software Tools for Technology Transfer},
	author = {Filiot, Emmanuel and Jin, Naiyong and Raskin, Jean-François},
	month = oct,
	year = {2013},
	pages = {541--561}
}

@incollection{maoz_two-way_2013,
	address = {Berlin, Heidelberg},
	title = {Two-{Way} {Traceability} and {Conflict} {Debugging} for {AspectLTL}
117
 {Programs}},
	isbn = {978-3-642-36964-3},
	Url_comment = {https://tmp_doi.org/10.1007/978-3-642-36964-3_2},
	abstract = {Tracing program actions back to the concerns that have caused them and blaming specific code artifacts for concern interference are known challenges of AOP and related advanced modularity paradigms. In this work, we address these challenges in the context of AspectLTL, a temporal-logic-based language for the specification and implementation of crosscutting concerns, which has a composition and synthesis-based weaving process whose output is a correct-by-construction executable artifact. When a specification is realizable, we provide two-way traceability information that links each allowed or forbidden transition in the generated program with the aspects that have justified its presence or elimination. When a specification is unrealizable, we provide an interactive game proof that demonstrates conflicts that should be fixed. The techniques are implemented and demonstrated using running examples.},
	booktitle = {Transactions on {Aspect}-{Oriented} {Software} {Development} {X}},
	publisher = {Springer Berlin Heidelberg},
	author = {Maoz, Shahar and Sa'ar, Yaniv},
	editor = {Leavens, Gary T. and Chiba, Shigeru and Tanter, Éric},
	year = {2013},
	tmp_doi_comment  = {10.1007/978-3-642-36964-3_2},
	pages = {39--72}
}

@article{sohail_safety_2013,
	title = {Safety first: a two-stage algorithm for the synthesis of reactive systems},
	volume = {15},
	issn = {1433-2787},
	Url_comment = {https://tmp_doi.org/10.1007/s10009-012-0224-3},
	tmp_doi_comment  = {10.1007/s10009-012-0224-3},
	abstract = {In the game-theoretic approach to the synthesis of reactive systems, specifications are often expressed as \$ømega\$-regular languages. Computing a winning strategy to an infinite game whose winning condition is an \$ømega\$-regular language is then the main step in obtaining an implementation. Conjoining all the properties of a specification to obtain a monolithic game suffers from the doubly exponential determinization that is required. Despite the success of symbolic algorithms, the monolithic approach is not practical. Existing techniques achieve efficiency by imposing restrictions on the \$ømega\$-regular languages they deal with. In contrast, we present an approach that achieves improvement in performance through the decomposition of the problem while still accepting the full set of \$ømega\$-regular languages. Each property is translated into a deterministic \$ømega\$-regular automaton explicitly while the two-player game defined by the collection of automata is played symbolically. Safety and persistence properties usually make up the majority of a specification. We take advantage of this by solving the game incrementally. Each safety and persistence property is used to gradually construct the parity game. Optimizations are applied after each refinement of the graph. This process produces a compact symbolic encoding of the parity game. We then compose the remaining properties and solve one final game after possibly solving smaller games to further optimize the graph. An implementation is finally derived from the winning strategies computed. We compare the results of our tool to those of the synthesis tool Anzu.},
	number = {5},
	journal = {International Journal on Software Tools for Technology Transfer},
	author = {Sohail, Saqib and Somenzi, Fabio},
	month = oct,
	year = {2013},
	pages = {433--454}
}

@article{joshi_denali:_2006,
	title = {Denali: {A} {Practical} {Algorithm} for {Generating} {Optimal} {Code}},
	volume = {28},
	issn = {0164-0925},
	Url_comment = {http://tmp_doi.acm.org/10.1145/1186632.1186633},
	tmp_doi_comment  = {10.1145/1186632.1186633},
	number = {6},
	journal = {ACM Trans. Program. Lang. Syst.},
	author = {Joshi, Rajeev and Nelson, Greg and Zhou, Yunhong},
	month = nov,
	year = {2006},
	keywords = {code generation, Compilation, practical optimal code generation},
	pages = {967--989}
}

@article{konighofer_debugging_2013,
	title = {Debugging formal specifications: a practical approach using model-based diagnosis and counterstrategies},
	volume = {15},
	issn = {1433-2787},
	Url_comment = {https://tmp_doi.org/10.1007/s10009-011-0221-y},
	tmp_doi_comment  = {10.1007/s10009-011-0221-y},
	abstract = {Creating a formal specification for a design is an error-prone process. At the same time, debugging incorrect specifications is difficult and time consuming. In this work, we propose a debugging method for formal specifications that does not require an implementation. We handle conflicts between a formal specification and the informal design intent using a simulation-based refinement loop, where we reduce the problem of debugging overconstrained specifications to that of debugging unrealizability. We show how model-based diagnosis can be applied to locate an error in an unrealizable specification. The diagnosis algorithm computes properties and signals that can be modified in such a way that the specification becomes realizable, thus pointing out potential error locations. In order to fix the specification, the user must understand the problem. We use counterstrategies to explain conflicts in the specification. Since counterstrategies may be large, we propose several ways to simplify them. First, we compute the counterstrategy not for the original specification but only for an unrealizable core. Second, we use a heuristic to search for a countertrace, i.e., a single input trace which necessarily leads to a specification violation. Finally, we present the countertrace or the counterstrategy as an interactive game against the user, and as a graph summarizing possible plays of this game. We introduce a user-friendly implementation of our debugging method and present experimental results for GR(1) specifications.},
	number = {5},
	journal = {International Journal on Software Tools for Technology Transfer},
	author = {Könighofer, Robert and Hofferek, Georg and Bloem, Roderick},
	month = oct,
	year = {2013},
	pages = {563--583}
}

@inproceedings{krishnamurthi_alchemy:_2008,
	address = {New York, NY, USA},
	series = {{SIGSOFT} '08/{FSE}-16},
	title = {Alchemy: {Transmuting} {Base} {Alloy} {Specifications} into {Implementations}},
	isbn = {978-1-59593-995-1},
	Url_comment = {http://tmp_doi.acm.org/10.1145/1453101.1453123},
	tmp_doi_comment  = {10.1145/1453101.1453123},
	booktitle = {Proceedings of the 16th {ACM} {SIGSOFT} {International} {Symposium} on {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Krishnamurthi, Shriram and Fisler, Kathi and Dougherty, Daniel J. and Yoo, Daniel},
	year = {2008},
	note = {event-place: Atlanta, Georgia},
	keywords = {alloy, program synthesis, relational specification},
	pages = {158--169}
}

@article{kuncak_functional_2013,
	title = {Functional synthesis for linear arithmetic and sets},
	volume = {15},
	issn = {1433-2787},
	Url_comment = {https://tmp_doi.org/10.1007/s10009-011-0217-7},
	tmp_doi_comment  = {10.1007/s10009-011-0217-7},
	abstract = {Synthesis of program fragments from specifications can make programs easier to write and easier to reason about. To integrate synthesis into programming languages, synthesis algorithms should behave in a predictable way—they should succeed for a well-defined class of specifications. To guarantee correctness and applicability to software (and not just hardware), these algorithms should also support unbounded data types, such as numbers and data structures. To obtain appropriate synthesis algorithms, we propose to generalize decision procedures into predictable and complete synthesis procedures. Such procedures are guaranteed to find the code that satisfies the specification if such code exists. Moreover, we identify conditions under which synthesis will statically decide whether the solution is guaranteed to exist and whether it is unique. We demonstrate our approach by starting from a quantifier elimination decision procedure for Boolean algebra of set with Presburger arithmetic and transforming it into a synthesis procedure. Our procedure also works in the presence of parametric coefficients. We establish results on the size and the efficiency of the synthesized code. We show that such procedures are useful as a language extension with implicit value definitions, and we show how to extend a compiler to support such definitions. Our constructs provide the benefits of synthesis to programmers, without requiring them to learn new concepts, give up a deterministic execution model, or provide code skeletons.},
	number = {5},
	journal = {International Journal on Software Tools for Technology Transfer},
	author = {Kuncak, Viktor and Mayer, Mikaël and Piskac, Ruzica and Suter, Philippe},
	month = oct,
	year = {2013},
	pages = {455--474}
}

@inproceedings{massalin_superoptimizer:_1987,
	address = {Los Alamitos, CA, USA},
	series = {{ASPLOS} {II}},
	title = {Superoptimizer: {A} {Look} at the {Smallest} {Program}},
	isbn = {0-8186-0805-6},
	Url_comment = {https://tmp_doi.org/10.1145/36206.36194},
	tmp_doi_comment  = {10.1145/36206.36194},
	booktitle = {Proceedings of the {Second} {International} {Conference} on {Architectual} {Support} for {Programming} {Languages} and {Operating} {Systems}},
	publisher = {IEEE Computer Society Press},
	author = {Massalin, Henry},
	year = {1987},
	note = {event-place: Palo Alto, California, USA},
	pages = {122--126}
}

@article{solar-lezama_program_2013,
	title = {Program sketching},
	volume = {15},
	issn = {1433-2787},
	Url_comment = {https://tmp_doi.org/10.1007/s10009-012-0249-7},
	tmp_doi_comment  = {10.1007/s10009-012-0249-7},
	abstract = {Sketching is a synthesis methodology that aims to bridge the gap between a programmer's high-level insights about a problem and the computer's ability to manage low-level details. In sketching, the programmer uses a partial program, a sketch, to describe the desired implementation strategy, and leaves the low-level details of the implementation to an automated synthesis procedure. In order to generate an implementation from the programmer provided sketch, the synthesizer uses counterexample-guided inductive synthesis (CEGIS). Inductive synthesis refers to the process of generating candidate implementations from concrete examples of correct or incorrect behavior. CEGIS combines a SAT-based inductive synthesizer with an automated validation procedure, a bounded model-checker, that checks whether the candidate implementation produced by inductive synthesis is indeed correct and to produce new counterexamples. The result is a synthesis procedure that is able to handle complex problems from a variety of domains including ciphers, scientific programs, and even concurrent data-structures.},
	number = {5},
	journal = {International Journal on Software Tools for Technology Transfer},
	author = {Solar-Lezama, Armando},
	month = oct,
	year = {2013},
	pages = {475--495}
}

@article{srivastava_template-based_2013,
	title = {Template-based program verification and program synthesis},
	volume = {15},
	issn = {1433-2787},
	Url_comment = {https://tmp_doi.org/10.1007/s10009-012-0223-4},
	tmp_doi_comment  = {10.1007/s10009-012-0223-4},
	abstract = {Program verification is the task of automatically generating proofs for a program's compliance with a given specification. Program synthesis is the task of automatically generating a program that meets a given specification. Both program verification and program synthesis can be viewed as search problems, for proofs and programs, respectively. For these search problems, we present approaches based on user-provided insights in the form of templates. Templates are hints about the syntactic forms of the invariants and programs, and help guide the search for solutions. We show how to reduce the template-based search problem to satisfiability solving, which permits the use of off-the-shelf solvers to efficiently explore the search space. Template-based approaches have allowed us to verify and synthesize programs outside the abilities of previous verifiers and synthesizers. Our approach can verify and synthesize difficult algorithmic textbook programs (e.g., sorting and dynamic programming-based algorithms) and difficult arithmetic programs.},
	number = {5},
	journal = {International Journal on Software Tools for Technology Transfer},
	author = {Srivastava, Saurabh and Gulwani, Sumit and Foster, Jeffrey S.},
	month = oct,
	year = {2013},
	pages = {497--518}
}

@book{steffen_module_1993,
	title = {Module {Configuration} by {Minimal} {Model} {Construction}},
	author = {Steffen, Bernhard and Margaria, Tiziana and Freitag, Burkhard},
	year = {1993}
}

@inproceedings{pnueli_distributed_1990,
	title = {Distributed {Reactive} {Systems} {Are} {Hard} to {Synthesize}},
	booktitle = {{FOCS}},
	author = {Pnueli, Amir and Rosner, Roni},
	year = {1990}
}

@article{church_logic_1964,
	title = {Logic, {Arithmetic}, and {Automata}},
	volume = {29},
	number = {4},
	journal = {Journal of Symbolic Logic},
	author = {Church, Alonzo},
	year = {1964},
	pages = {210--210}
}

@article{bloem_synthesis_2012,
	title = {Synthesis of {Reactive}(1) designs},
	volume = {78},
	issn = {0022-0000},
	Url_comment = {http://www.sciencedirect.com/science/article/pii/S0022000011000869},
	tmp_doi_comment  = {https://tmp_doi.org/10.1016/j.jcss.2011.08.007},
	number = {3},
	journal = {Journal of Computer and System Sciences},
	author = {Bloem, Roderick and Jobstmann, Barbara and Piterman, Nir and Pnueli, Amir and Saʼar, Yaniv},
	year = {2012},
	keywords = {Game theory, Property synthesis, Realizability},
	pages = {911 -- 938},
	annote = {In Commemoration of Amir Pnueli}
}

@article{wessel_generic_2013,
	title = {Generic mapping tools: improved version released},
	volume = {94},
	number = {45},
	journal = {EOS Trans. Amer. Geophys. U.},
	author = {Wessel, Paul and Smith, Walter HF and Scharroo, Remko and {others}},
	year = {2013},
	pages = {409--410}
}

@article{reza_collaborative_nodate,
	title = {Collaborative {Ontology} {Development} for the {Geosciences}},
	volume = {18},
	abstract = {Abstract Ontology‐based information publishing, retrieval, reuse, and integration have become popular research topics to address the challenges involved in exchanging data between heterogeneous sources. However, in most cases ontologies are still developed in a centralized top‐down manner by a few knowledge engineers. Consequently, the role that developers play in conceptualizing a domain such as the geosciences is disproportional compared with the role of domain experts and especially potential end‐users. These and other drawbacks have stimulated the creation of new methodologies focusing around collaboration. Based on a review of existing approaches, this article presents a two‐step methodology and implementation to foster collaborative ontology engineering in the geosciences. Our approach consists of the development of a minimalistic core ontology acting as a catalyst and the creation of a virtual collaborative development cycle. Both methodology and prototypical implementation have been tested in the context of the EU‐funded ForeStClim project which addresses environmental protection with respect to forests and climate change.},
	number = {6},
	journal = {Transactions in GIS},
	author = {Reza, Kalbasi and Krzysztof, Janowicz and Femke, Reitsma and Luc, Boerboom and Ali, Alesheikh},
	pages = {834--851}
}

@article{albrecht_geo-ontology_2008,
	title = {Geo-ontology {Tools}: {The} {Missing} {Link}},
	volume = {12},
	journal = {T. GIS},
	author = {Albrecht, Jochen and Derman, Brandon and Ramasubramanian, Laxmi},
	year = {2008},
	pages = {409--424}
}

@inproceedings{cox_explicit_2013,
	address = {Aachen, Germany, Germany},
	series = {{SSN}'13},
	title = {An {Explicit} {OWL} {Representation} of {ISO}/{OGC} {Observations} and {Measurements}},
	booktitle = {Proceedings of {SSN} 13 - {Volume} 1063},
	publisher = {CEUR-WS.org},
	author = {Cox, Simon J. D.},
	year = {2013},
	note = {event-place: Sydney, Australia},
	keywords = {ISO, observation, OGC, ontology alignment, OWL, semantic sensor network, sensor, UML},
	pages = {1--18}
}

@techreport{lehmann_geoknow_2015,
	title = {The {GeoKnow} {Handbook}},
	author = {Lehmann, Jens and Athanasiou, Spiros and Both, Andreas and {others}},
	year = {2015},
	keywords = {2015 MOLE buehmann geoknow group\_aksw hoeffner lehmann ngonga sherif westphal}
}

@article{dodge_leatherback_2014,
	title = {Leatherback {Turtle} {Movements}, {Dive} {Behavior}, and {Habitat} {Characteristics} in {Ecoregions} of the {Northwest} {Atlantic} {Ocean}},
	volume = {9},
	issn = {1932-6203},
	tmp_doi_comment  = {10.1371/journal.pone.0091726},
	abstract = {Leatherback sea turtles, Dermochelys coriacea, are highly migratory predators that feed exclusively on gelatinous zooplankton, thus playing a unique role in coastal and pelagic food webs. From 2007 to 2010, we used satellite telemetry to monitor the movements and dive behavior of nine adult and eleven subadult leatherbacks captured on the Northeast USA shelf and tracked throughout the Northwest Atlantic. Leatherback movements and environmental associations varied by oceanographic region, with slow, sinuous, area-restricted search behavior and shorter, shallower dives occurring in cool (median sea surface temperature: 18.4°C), productive (median chlorophyll a: 0.80 mg m−3), shallow (median bathymetry: 57 m) shelf habitat with strong sea surface temperature gradients (median SST gradient: 0.23°C km−1) at temperate latitudes. Leatherbacks were highly aggregated in temperate shelf and slope waters during summer, early fall, and late spring and more widely dispersed in subtropical and tropical oceanic and neritic habitat during late fall, winter and early spring. We investigated the relationship of ecoregion, satellite-derived surface chlorophyll, satellite-derived sea surface temperature, SST gradient, chlorophyll gradient and bathymetry with leatherback search behavior using generalized linear mixed-effects models. The most well supported model showed that differences in leatherback search behavior were best explained by ecoregion and regional differences in bathymetry and SST. Within the Northwest Atlantic Shelves region, leatherbacks increased path sinuosity (i.e., looping movements) with increasing SST, but this relationship reversed within the Gulf Stream region. Leatherbacks increased path sinuosity with decreasing water depth in temperate and tropical shelf habitats. This relationship is consistent with increasing epipelagic gelatinous zooplankton biomass with decreasing water depth, and bathymetry may be a key feature in identifying leatherback foraging habitat in neritic regions. High-use habitat for leatherbacks in our study occurred in coastal waters of the North American eastern seaboard and eastern Caribbean, putting turtles at heightened risk from land- and ocean-based human activity.},
	number = {3},
	urldate = {2019-01-22},
	journal = {PLoS ONE},
	author = {Dodge, Kara L. and Galuardi, Benjamin and Miller, Timothy J. and {others}},
	month = mar,
	year = {2014},
	pmid = {24646920},
	pmcid = {PMC3960146},
	keywords = {animal movements},
	file = {PubMed Central Full Text PDF:/home/vedran/Zotero/storage/JPQP2AKF/Dodge et al. - 2014 - Leatherback Turtle Movements, Dive Behavior, and H.pdf:application/pdf}
}

@book{noauthor_data_2017,
	title = {Data from: {Movement} patterns of a keystone waterbird species are highly predictable from landscape configuration},
	copyright = {license\_1},
	shorttitle = {Data from},
	Url_comment = {http://hdl.handle.net/10255/move.644},
	abstract = {Background: Movement behaviour is fundamental to the ecology of animals and their interactions with other organisms, and as such contributes to ecosystem dynamics. Waterfowl are key players in ecological processes in wetlands and surrounding habitats through predator-prey interactions and their transportation of nutrients and other organisms. Understanding the drivers of their movement behaviour is crucial to predict how environmental changes affect their role in ecosystem functioning. Mallards (Anas platyrhynchos) are the most abundant duck species worldwide and important dispersers of aquatic invertebrates, plants and pathogens like avian influenza viruses. By GPS tracking of 97 mallards in four landscape types along a gradient of wetland availability, we identified patterns in their daily movement behaviour and quantified potential effects of weather conditions and water availability on the spatial scale of their movements. Results: We demonstrate that mallard movement patterns were highly predictable, with regular commuting flights at dusk and dawn between a fixed day roost and one or several fixed nocturnal foraging sites, linked strongly to surface water. Wind and precipitation hardly affected movement, but flight distances and home range sizes increased when temperatures dropped towards zero. Flight distances and home range sizes increased exponentially with decreasing availability of freshwater habitat. Total shoreline length and the number of water bodies in the landscape surrounding the roost were the best predictors of the spatial scale of daily mallard movements. Conclusions: Our results show how mallards may flexibly adjust the spatial scale of their movements to wetland availability in the landscape. This implies that mallards moving between discrete habitat patches continue to preserve biotic connectivity in increasingly fragmented landscapes. The high predictability of mallard movement behaviour in relation to landscape features makes them reliable dispersal vectors for organisms to adapt to, and allows prediction of their ecological role in other landscapes.},
	language = {eng},
	year = {2017},
	keywords = {Anas platyrhynchos, animal movement, animal tracking, dispersal, ecological connectivity, habitat fragmentation, home range, land use change, landscape configuration, mallard, movement ecology},
	file = {Data from\: Movement patterns of a keystone waterbird species are highly predictable from landscape configuration:/home/vedran/Zotero/storage/J8T9B4KQ/move.html:text/html}
}

@article{kleyheeg_movement_2017,
	title = {Movement patterns of a keystone waterbird species are highly predictable from landscape configuration},
	volume = {5},
	issn = {2051-3933},
	tmp_doi_comment  = {10.1186/s40462-016-0092-7},
	abstract = {Movement behaviour is fundamental to the ecology of animals and their interactions with other organisms, and as such contributes to ecosystem dynamics. Waterfowl are key players in ecological processes in wetlands and surrounding habitats through predator-prey interactions and their transportation of nutrients and other organisms. Understanding the drivers of their movement behaviour is crucial to predict how environmental changes affect their role in ecosystem functioning. Mallards (Anas platyrhynchos) are the most abundant duck species worldwide and important dispersers of aquatic invertebrates, plants and pathogens like avian influenza viruses. By GPS tracking of 97 mallards in four landscape types along a gradient of wetland availability, we identified patterns in their daily movement behaviour and quantified potential effects of weather conditions and water availability on the spatial scale of their movements.},
	number = {1},
	urldate = {2019-02-05},
	journal = {Movement Ecology},
	author = {Kleyheeg, Erik and van Dijk, Jacintha G. B. and Tsopoglou-Gkina, Despina and {others}},
	month = feb,
	year = {2017},
	keywords = {Mallard movements in the Netherlands},
	pages = {2}
}

@article{coyne_satellite_2005,
	title = {Satellite {Tracking} and {Analysis} {Tool} ({STAT}): an integrated system for archiving, analyzing and mapping animal tracking data},
	volume = {301},
	issn = {0171-8630, 1616-1599},
	shorttitle = {Satellite {Tracking} and {Analysis} {Tool} ({STAT})},
	tmp_doi_comment  = {10.3354/meps301001},
	abstract = {Despite the obvious power and advantages of the Argos system to track animals by satellite, the data generated are difficult for many biologists to exploit. A broad range of skills is required to efficiently download, collate, filter and interpret Argos data. Integration of animal movements with other physical (e.g. remote sensing imagery) and anthropogenic (e.g. fishery distributions) datasets presents additional technical and computing challenges. The Satellite Tracking and Analysis Tool (STAT) is a freely available system designed for biologists who work on animal tracking; it includes a set of standardized tools and techniques for data management, analysis, and integration with environmental data. STAT logs in to the Argos computer network each day and downloads all available locations and associated data for each user. These data are parsed and stored in a relational database and automatically backed up to an offsite location. A number of data filtering options are available, including setting maximum speed, time or distance between consecutive points, Argos location class, and turning angle. A variety of environmental data layers, including bathymetry, sea surface temperature, sea surface height, ocean currents and chlorophyll, can be sampled for all locations in the STAT database and can be downloaded and incorporated into tracking maps and animations. STAT also facilitates collaboration and the sharing of animal tracking information with the wider public and funding organizations. We hope that STAT will act as a catalytic foundation, fostering collaboration among users of satellite telemetry, and ensuring maximum value from these studies.},
	language = {en},
	urldate = {2019-01-22},
	journal = {Marine Ecology Progress Series},
	author = {Coyne, M. S. and Godley, B. J.},
	month = oct,
	year = {2005},
	keywords = {Argos, Biologging, Oceanography, Satellite tracking, Wildlife telemetry},
	pages = {1--7}
}

@article{gil_wings:_2011,
	title = {Wings: {Intelligent} {Workflow}-{Based} {Design} of {Computational} {Experiments}},
	volume = {26},
	issn = {1541-1672},
	number = {1},
	journal = {IEEE Intelligent Systems},
	author = {Gil, Y. and Ratnakar, V. and Kim, J. and {others}},
	month = jan,
	year = {2011},
	keywords = {Al planning, computational experiments, Computational intelligence, computer-supported discovery, data analysis, data set requirement, Design methodology, experiment design, Information Sciences Institute, intelligent systems, natural sciences computing, planning (artificial intelligence), software architecture, software components, Tracking, University of Southern California, workflow creation, workflow execution, workflow instance generation and specialization, workflow management, workflow management software, Workflow management software, workflow system, workflow validation},
	pages = {62--72}
}

@book{johnston_using_2001,
	title = {Using {ArcGIS} geostatistical analyst},
	volume = {380},
	publisher = {Esri Redlands},
	author = {Johnston, Kevin and Ver Hoef, Jay M and Krivoruchko, Konstantin and Lucas, Neil},
	year = {2001}
}

@article{kranstauber_movebank_2011,
	title = {The {Movebank} data model for animal tracking},
	volume = {26},
	issn = {1364-8152},
	tmp_doi_comment  = {10.1016/j.envsoft.2010.12.005},
	abstract = {Studies of animal movement are rapidly increasing as tracking technologies make it possible to collect more data of a larger variety of species. Comparisons of animal movement across sites, times, or species are key to asking questions about animal adaptation, responses to climate and land-use change. Thus, great gains can be made by sharing and exchanging animal tracking data. Here we present an animal movement data model that we use within the Movebank web application to describe tracked animals. The model facilitates data comparisons across a broad range of taxa, study designs, and technologies, and is based on the scientific questions that could be addressed with the data.},
	number = {6},
	urldate = {2019-02-15},
	journal = {Environmental Modelling \& Software},
	author = {Kranstauber, B. and Cameron, A. and Weinzerl, R. and Fountain, T. and Tilak, S. and Wikelski, M. and Kays, R.},
	month = jun,
	year = {2011},
	keywords = {Animal movement, Argos, Data model, GPS, Movebank, Tracking, VHF Telemetry},
	pages = {834--835},
	file = {ScienceDirect Full Text PDF:/home/vedran/Zotero/storage/WC2JLNQG/Kranstauber et al. - 2011 - The Movebank data model for animal tracking.pdf:application/pdf;ScienceDirect Snapshot:/home/vedran/Zotero/storage/3ARNIIG8/S1364815210003257.html:text/html}
}

@inproceedings{lamprechtKasalica2018,
    author = {Kasalica, Vedran and Lamprecht, Anna-Lena},
    year = {2018},
    month = {10},
    pages = {362-363},
    title = {Automated Composition of Scientific Workflows: A Case Study on Geographic Data Manipulation},
    tmp_doi_comment = {10.1109/eScience.2018.00099}
}

@article{Biere2003BoundedMC,
  title={Bounded model checking},
  author={Armin Biere and Alessandro Cimatti and Edmund M. Clarke and Ofer Strichman and Yunshan Zhu},
  journal={Advances in Computers},
  year={2003},
  volume={58},
  pages={117-148}
}

@inproceedings{green_application_1969,
	address = {San Francisco, CA, USA},
	series = {{IJCAI}'69},
	title = {Application of {Theorem} {Proving} to {Problem} {Solving}},
	url_comment = {http://dl.acm.org/citation.cfm?id=1624562.1624585},
	abstract = {This paper shows how an extension of the resolution proof procedure can be used to construct problem solutions. The extended proof procedure can solve problems involving state transformations. The paper explores several alternate problem representations and provides a discussion of solutions to sample problems including the "Monkey and Bananas" puzzle and the 'Tower of Hanoi" puzzle. The paper exhibits solutions to these problems obtained by QA3, a computer program bused on these theorem-proving methods. In addition, the paper shows how QA3 can write simple computer programs and can solve practical problems for a simple robot.},
	urldate = {2019-03-25},
	booktitle = {Proceedings of the 1st {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Green, Cordell},
	year = {1969},
	note = {event-place: Washington, DC},
	keywords = {automatic programming, problem solving, program writing, question answering, resolution, robots, state transformations, theorem proving},
	pages = {219--239}
}

@article{manna_toward_1971,
	title = {Toward {Automatic} {Program} {Synthesis}},
	volume = {14},
	issn = {0001-0782},
	url_comment = {http://tmp_doi.acm.org/10.1145/362566.362568},
	tmp_doi_comment = {10.1145/362566.362568},
	abstract = {An elementary outline of the theorem-proving approach to automatic program synthesis is given, without dwelling on technical details. The method is illustrated by the automatic construction of both recursive and iterative programs operating on natural numbers, lists, and trees.
In order to construct a program satisfying certain specifications, a theorem induced by those specifications is proved, and the desired program is extracted from the proof. The same technique is applied to transform recursively defined functions into iterative programs, frequently with a major gain in efficiency.
It is emphasized that in order to construct a program with loops or with recursion, the principle of mathematical induction must be applied. The relation between the version of the induction rule used and the form of the program constructed is explored in some detail.},
	number = {3},
	urldate = {2019-03-25},
	journal = {Commun. ACM},
	author = {Manna, Zohar and Waldinger, Richard J.},
	month = mar,
	year = {1971},
	keywords = {answer extraction, artificial intelligence, automatic program synthesis, mathematical induction principle, problem solving, theorem proving},
	pages = {151--165},
	file = {ACM Full Text PDF:/home/vedran/Zotero/storage/MNN77MEY/Manna and Waldinger - 1971 - Toward Automatic Program Synthesis.pdf:application/pdf}
}

@article{manna_knowledge_1975,
	title = {Knowledge and reasoning in program synthesis},
	volume = {6},
	issn = {0004-3702},
	url_comment = {http://www.sciencedirect.com/science/article/pii/0004370275900089},
	tmp_doi_comment = {10.1016/0004-3702(75)90008-9},
	abstract = {Program synthesis is the construction of a computer program from given specifications. An automatic program synthesis system must combine reasoning and programming ability with a good deal of knowledge about the subject matter of the program. This ability and knowledge must be manifested both procedurally (by programs) and structurally (by choice of representation). We describe some of the reasoning and programming capabilities of a projected synthesis system. Special attention is paid to the introduction of conditional tests, loops, and instructions with side effects in the program being constructed. The ability to satisfy several interacting goals simultaneously proves to be important in many contexts. The modification of an already existing program to solve a somewhat different problem has been found to be a powerful approach. We illustrate these concepts with hand simulations of the synthesis of a number of pattern-matching programs. Some of these techniques have already been implemented, some are in the course of implementation, while others seem equivalent to well-known unsolved problems in artificial intelligence.},
	number = {2},
	urldate = {2019-03-25},
	journal = {Artificial Intelligence},
	author = {Manna, Zohar and Waldinger, Richard},
	month = jun,
	year = {1975},
	pages = {175--208},
	file = {ScienceDirect Full Text PDF:/home/vedran/Zotero/storage/KIAKYLR4/Manna and Waldinger - 1975 - Knowledge and reasoning in program synthesis.pdf:application/pdf;ScienceDirect Snapshot:/home/vedran/Zotero/storage/48JIW89X/0004370275900089.html:text/html}
}

@inproceedings{gulwani_dimensions_2010,
	address = {New York, NY, USA},
	series = {{PPDP} '10},
	title = {Dimensions in {Program} {Synthesis}},
	isbn = {978-1-4503-0132-9},
	url_comment = {http://tmp_doi.acm.org/10.1145/1836089.1836091},
	tmp_doi_comment = {10.1145/1836089.1836091},
	abstract = {Program Synthesis, which is the task of discovering programs that realize user intent, can be useful in several scenarios: enabling people with no programming background to develop utility programs, helping regular programmers automatically discover tricky/mundane details, program understanding, discovery of new algorithms, and even teaching. This paper describes three key dimensions in program synthesis: expression of user intent, space of programs over which to search, and the search technique. These concepts are illustrated by brief description of various program synthesis projects that target synthesis of a wide variety of programs such as standard undergraduate textbook algorithms e.g., sorting, dynamic programming), program inverses(e.g., decoders, deserializers), bitvector manipulation routines, deobfuscated programs, graph algorithms, text-manipulating routines, mutual exclusion algorithms, etc.},
	urldate = {2019-03-21},
	booktitle = {Proceedings of the 12th {International} {ACM} {SIGPLAN} {Symposium} on {Principles} and {Practice} of {Declarative} {Programming}},
	publisher = {ACM},
	author = {Gulwani, Sumit},
	year = {2010},
	note = {event-place: Hagenberg, Austria},
	keywords = {belief propagation, deductive synthesis, genetic programming, inductive synthesis, machine learning, probabilistic inference, programming by demonstration, programming by examples, sat solving, smt solving},
	pages = {13--24},
	file = {ACM Full Text PDF:/home/vedran/Zotero/storage/HH6YDTEK/Gulwani - 2010 - Dimensions in Program Synthesis.pdf:application/pdf}
}

@inproceedings{alur_syntax_guided_2013,
	title = {Syntax-guided synthesis},
	tmp_doi_comment = {10.1109/FMCAD.2013.6679385},
	abstract = {The classical formulation of the program-synthesis problem is to find a program that meets a correctness specification given as a logical formula. Recent work on program synthesis and program optimization illustrates many potential benefits of allowing the user to supplement the logical specification with a syntactic template that constrains the space of allowed implementations. Our goal is to identify the core computational problem common to these proposals in a logical framework. The input to the syntax-guided synthesis problem (SyGuS) consists of a background theory, a semantic correctness specification for the desired program given by a logical formula, and a syntactic set of candidate implementations given by a grammar. The computational problem then is to find an implementation from the set of candidate expressions so that it satisfies the specification in the given theory. We describe three different instantiations of the counter-example-guided-inductive-synthesis (CEGIS) strategy for solving the synthesis problem, report on prototype implementations, and present experimental results on an initial set of benchmarks.},
	booktitle = {2013 {Formal} {Methods} in {Computer}-{Aided} {Design}},
	author = {Alur, R. and Bodik, R. and Juniwal, G. and Martin, M. M. K. and Raghothaman, M. and Seshia, S. A. and Singh, R. and Solar-Lezama, A. and Torlak, E. and Udupa, A.},
	month = oct,
	year = {2013},
	keywords = {automatic programming, CEGIS, computational linguistics, computational problem, Concrete, counter-example-guided-inductive-synthesis strategy, formal specification, Grammar, Heuristic algorithms, Libraries, logical formula, logical specification, Production, program optimization, program verification, program-synthesis problem, Search problems, semantic correctness specification, SyGuS, syntactic template, Syntactics, syntax-guided synthesis problem},
	pages = {1--8},
	file = {IEEE Xplore Abstract Record:/home/vedran/Zotero/storage/JJ2CETS7/6679385.html:text/html;Submitted Version:/home/vedran/Zotero/storage/KPBIBZ9F/Alur et al. - 2013 - Syntax-guided synthesis.pdf:application/pdf}
}

@phdthesis{solar-lezama_program_2008,
	address = {Berkeley, CA, USA},
	type = {{PhD} {Thesis}},
	title = {Program {Synthesis} by {Sketching}},
	abstract = {The goal of software synthesis is to generate programs automatically from high-level specifications. However, efficient implementations for challenging programs require a combination of high-level algorithmic insights and low-level implementation details. Deriving the low-level details is a natural job for a computer, but the synthesizer can not replace the human insight. Therefore, one of the central challenges for software synthesis is to establish a synergy between the programmer and the synthesizer, exploiting the programmer's expertise to reduce the burden on the synthesizer. This thesis introduces sketching, a new style of synthesis that offers a fresh approach to the synergy problem. Previous approaches have relied on meta-programming, or variations of interactive theorem proving to help the synthesizer deduce an efficient implementation. The resulting systems are very powerful, but they require the programmer to master new formalisms far removed from traditional programming models. To make synthesis accessible, programmers must be able to provide their insight effortlessly, using formalisms they already understand. In Sketching, insight is communicated through a partial program, a  sketch that expresses the high-level structure of an implementation but leaves holes in place of the low-level details. This form of synthesis is made possible by a new SAT-based inductive synthesis procedure that can efficiently synthesize an implementation from a small number of test cases. This algorithm forms the core of a new counterexample guided inductive synthesis procedure (CEGIS) which combines the inductive synthesizer with a validation procedure to automatically generate test inputs and ensure that the generated program satisfies its specification. With a few extensions, CEGIS can even use its sequential inductive synthesizer to generate concurrent programs; all the concurrency related reasoning is delegated to an off-the-shelf validation procedure. The resulting synthesis system scales to real programming problems from a variety of domains ranging from bit-level ciphers to manipulations of linked datastructures. The system was even used to produce a complete optimized implementation of the AES cipher. The concurrency aware synthesizer was also used to synthesize, in a matter of minutes, the details of a fine-locking scheme for a concurrent set, a sense reversing barrier, and even a solution to the dining philosophers problem. The system was also extended with domain specific knowledge to better handle the problem of implementing stencil computations, an important domain in scientific computing. For this domain, we were able to encode domain specific insight as a problem reduction that converted stencil sketches into simplified sketch problems which CEGIS resolved in a matter of minutes. This specialized synthesizer was used to quickly implement a MultiGrid solver for partial differential equations containing many difficult implementation strategies from the literature. In short, this thesis shows that sketching is a viable approach to making synthesis practical in a general programming context.},
	school = {University of California at Berkeley},
	author = {Solar-Lezama, Armando},
	year = {2008},
	keywords = {CEGIS},
	annote = {AAI3353225}
}


@article{phothilimthana_scaling_2016,
	title = {Scaling up {Superoptimization}},
	volume = {50},
	issn = {0163-5980, 0362-1340, 0163-5964},
	url_comment = {http://dl.acm.org/citation.cfm?id=2954680.2872387},
	tmp_doi_comment = {10.1145/2954680.2872387},
	number = {2},
	urldate = {2019-03-21},
	journal = {ACM SIGOPS Operating Systems Review},
	author = {Phothilimthana, Phitchaya Mangpo and Thakur, Aditya and Bodik, Rastislav  and {others}},
	month = jun,
	year = {2016},
	pages = {297--310},
	annote = {LENS algorithm},
	file = {Full Text:/home/vedran/Zotero/storage/VB2T9NX3/Phothilimthana et al. - 2016 - Scaling up Superoptimization.pdf:application/pdf;Snapshot:/home/vedran/Zotero/storage/E4P6JKKX/citation.html:text/html}
}

@inproceedings{gulwani_synthesis_2011,
	address = {New York, NY, USA},
	series = {{PLDI} '11},
	title = {Synthesis of {Loop}-free {Programs}},
	isbn = {978-1-4503-0663-8},
	url_comment = {http://tmp_doi.acm.org/10.1145/1993498.1993506},
	tmp_doi_comment = {10.1145/1993498.1993506},
	abstract = {We consider the problem of synthesizing loop-free programs that implement a desired functionality using components from a given library. Specifications of the desired functionality and the library components are provided as logical relations between their respective input and output variables. The library components can be used at most once, and hence the library is required to contain a reasonable overapproximation of the multiset of the components required. We solve the above component-based synthesis problem using a constraint-based approach that involves first generating a synthesis constraint, and then solving the constraint. The synthesis constraint is a first-order ∃∀ logic formula whose size is quadratic in the number of components. We present a novel algorithm for solving such constraints. Our algorithm is based on counterexample guided iterative synthesis paradigm and uses off-the-shelf SMT solvers. We present experimental results that show that our tool Brahma can efficiently synthesize highly nontrivial 10-20 line loop-free bitvector programs. These programs represent a state space of approximately 2010 programs, and are beyond the reach of the other tools based on sketching and superoptimization.},
	urldate = {2019-03-21},
	booktitle = {Proceedings of the 32Nd {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {ACM},
	author = {Gulwani, Sumit and Jha, Susmit and Tiwari, Ashish and Venkatesan, Ramarathnam},
	year = {2011},
	note = {event-place: San Jose, California, USA},
	keywords = {program synthesis, component-based synthesis, smt},
	pages = {62--73},
	file = {ACM Full Text PDF:/home/vedran/Zotero/storage/MRVXKKAC/Gulwani et al. - 2011 - Synthesis of Loop-free Programs.pdf:application/pdf}
}

@inproceedings{dijkstra_program_1979,
	address = {London, UK, UK},
	title = {Program {Inversion}},
	isbn = {978-3-540-09251-3},
	url_comment = {http://dl.acm.org/citation.cfm?id=647639.733360},
	urldate = {2019-03-25},
	booktitle = {Program {Construction}, {International} {Summer} {School}},
	publisher = {Springer-Verlag},
	author = {Dijkstra, Edsger W.},
	year = {1979},
	pages = {54--57}
}

@inproceedings{srivastava_program_2010,
	address = {Madrid, Spain},
	title = {From program verification to program synthesis},
	booktitle = {Proceedings of the 37th {ACM} {SIGPLAN}-{SIGACT} {Symposium} on {Principles} of {Programming} {Languages} ({POPL} 2010)},
	publisher = {ACM},
	author = {Srivastava, Saurabh and Gulwani, Sumit and Foster, Jeffrey S.},
	year = {2010},
	pages = {313--326}
}

@incollection{gulwani_programming_2016,
	title = {Programming by examples (and its applications in data wrangling)},
	abstract = {Programming by Examples (PBE) has the potential to revolutionize enduser programming by enabling end users, most of whom are non-programmers, to create scripts for automating repetitive tasks. PBE involves synthesizing intended programs in an underlying domain-specific language (DSL) from example based specifications (Ispec).We formalize the notion of Ispec and discuss some principles behind designing useful DSLs for synthesis. A key technical challenge in PBE is to search for programs that are consistent with the Ispec provided by the user. We present a divide-and-conquer based search paradigm that leverages deductive rules and version space algebras for manipulating sets of programs. Another technical challenge in PBE is to resolve the ambiguity that is inherent in the Ispec. We show how machine learning based ranking techniques can be used to predict an intended program within a set of programs that are consistent with the Ispec. We also present some user interaction models including program navigation and active-learning based conversational clarification that communicate actionable information to the user to help resolve ambiguity in the Ispec. The above-mentioned concepts are illustrated using practical PBE systems for data wrangling (including FlashFill, FlashExtract, FlashRelate), several of which have already been deployed in the real world.},
	author = {Gulwani, S},
	month = apr,
	year = {2016},
	tmp_doi_comment = {10.3233/978-1-61499-627-9-137},
	pages = {137--158},
	annote = {data wrangling}
}

@inproceedings{gulwani_automating_2011,
	address = {New York, NY, USA},
	series = {{POPL} '11},
	title = {Automating {String} {Processing} in {Spreadsheets} {Using} {Input}-output {Examples}},
	isbn = {978-1-4503-0490-0},
	url_comment = {http://tmp_doi.acm.org/10.1145/1926385.1926423},
	tmp_doi_comment = {10.1145/1926385.1926423},
	abstract = {We describe the design of a string programming/expression language that supports restricted forms of regular expressions, conditionals and loops. The language is expressive enough to represent a wide variety of string manipulation tasks that end-users struggle with. We describe an algorithm based on several novel concepts for synthesizing a desired program in this language from input-output examples. The synthesis algorithm is very efficient taking a fraction of a second for various benchmark examples. The synthesis algorithm is interactive and has several desirable features: it can rank multiple solutions and has fast convergence, it can detect noise in the user input, and it supports an active interaction model wherein the user is prompted to provide outputs on inputs that may have multiple computational interpretations. The algorithm has been implemented as an interactive add-in for Microsoft Excel spreadsheet system. The prototype tool has met the golden test - it has synthesized part of itself, and has been used to solve problems beyond author's imagination.},
	urldate = {2019-03-25},
	booktitle = {Proceedings of the 38th {Annual} {ACM} {SIGPLAN}-{SIGACT} {Symposium} on {Principles} of {Programming} {Languages}},
	publisher = {ACM},
	author = {Gulwani, Sumit},
	year = {2011},
	note = {event-place: Austin, Texas, USA},
	keywords = {program synthesis, programming by example (pbe), spreadsheet programming, string manipulation, user intent, version space algebra},
	pages = {317--330},
	file = {ACM Full Text PDF:/home/vedran/Zotero/storage/BW8XGWV3/Gulwani - 2011 - Automating String Processing in Spreadsheets Using.pdf:application/pdf}
}

@article{mo_learning_1992,
	title = {Learning text editing tasks from examples: a procedural approach},
	volume = {11},
	issn = {0144-929X},
	shorttitle = {Learning text editing tasks from examples},
	url_comment = {https://tmp_doi.org/10.1080/01449299208924317},
	tmp_doi_comment = {10.1080/01449299208924317},
	abstract = {Reformatting blocks of semi-structured information is a common editing task that typically involves highly repetitive action sequences, but ones where exceptional cases arise constantly and must be dealt with as they arise. This paper describes a procedural programming-by-example approach to repetitive text editing which allows users to construct programs within a standard editing interface and extend them incrementally. Following a brief practice period during which they settle on an editing strategy for the task at hand, users commence editing in the normal way. Once the first block of text has been edited, they inform the learning system which constructs a generalized procedure from the actions that have been recorded. The system then attempts to apply the procedure to the next block of text, by predicting editing actions and displaying them for confirmation. If the user accepts a prediction, the action is carried out (and the program may be generalized accordingly); otherwise the user is asked to intervene and supply additional information, in effect debugging the program on the fly. A pilot implementation is described that operates in a simple interactive point-and-click editor (Macintosh MINI-EDIT), along with its performance on three sample tasks. In one case the procedure was learned correctly from the actions on the first text block, while in the others minor debugging was needed on subsequent text blocks. In each case a much smaller number of both keystrokes and mouse-clicks was required than with normal editing, without the system making any prior assumptions about the stucture of the text except for some general knowledge about lexical patterns. Although a smooth interactive interface has not yet been constructed, the results obtained serve to indicate the potential of this approach for semi-structured editing tasks.},
	number = {1},
	urldate = {2019-03-22},
	journal = {Behaviour \& Information Technology},
	author = {MO, DAN H. and WITTEN, IAN H.},
	month = jan,
	year = {1992},
	pages = {32--45},
	annote = {A simple approach used for text editing on principle of programming-by-example. Where system suggest solutions based on the provided examples, and user interacts with the system in order to confirm or improves the suggested solutions.},
	file = {Snapshot:/home/vedran/Zotero/storage/CQM4P5AW/01449299208924317.html:text/html;Submitted Version:/home/vedran/Zotero/storage/N5FTLPF8/MO and WITTEN - 1992 - Learning text editing tasks from examples a proce.pdf:application/pdf}
}

@inproceedings{lau_version_2000,
	address = {San Francisco, CA, USA},
	series = {{ICML} '00},
	title = {Version {Space} {Algebra} and {Its} {Application} to {Programming} by {Demonstration}},
	isbn = {978-1-55860-707-1},
	url_comment = {http://dl.acm.org/citation.cfm?id=645529.657973},
	abstract = {An abstract is not available.},
	urldate = {2019-03-22},
	booktitle = {Proceedings of the {Seventeenth} {International} {Conference} on {Machine} {Learning}},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Lau, Tessa A. and Domingos, Pedro and Weld, Daniel S.},
	year = {2000},
	pages = {527--534},
	file = {Lau et al. - 2000 - Version Space Algebra and Its Application to Progr.pdf:/home/vedran/Zotero/storage/L552NB85/Lau et al. - 2000 - Version Space Algebra and Its Application to Progr.pdf:application/pdf}
}

@inproceedings{mandelin_jungloid_2005,
	address = {New York, NY, USA},
	series = {{PLDI} '05},
	title = {Jungloid {Mining}: {Helping} to {Navigate} the {API} {Jungle}},
	isbn = {978-1-59593-056-9},
	shorttitle = {Jungloid {Mining}},
	url_comment = {http://tmp_doi.acm.org/10.1145/1065010.1065018},
	tmp_doi_comment = {10.1145/1065010.1065018},
	abstract = {Reuse of existing code from class libraries and frameworks is often difficult because APIs are complex and the client code required to use the APIs can be hard to write. We observed that a common scenario is that the programmer knows what type of object he needs, but does not know how to write the code to get the object.In order to help programmers write API client code more easily, we developed techniques for synthesizing jungloid code fragments automatically given a simple query that describes that desired code in terms of input and output types. A jungloid is simply a unary expression; jungloids are simple, enabling synthesis, but are also versatile, covering many coding problems, and composable, combining to form more complex code fragments. We synthesize jungloids using both API method signatures and jungloids mined from a corpus of sample client programs.We implemented a tool, prospector, based on these techniques. prospector is integrated with the Eclipse IDE code assistance feature, and it infers queries from context so there is no need for the programmer to write queries. We tested prospector on a set of real programming problems involving APIs; prospector found the desired solution for 18 of 20 problems. We also evaluated prospector in a user study, finding that programmers solved programming problems more quickly and with more reuse when using prospector than without prospector.},
	urldate = {2019-03-22},
	booktitle = {Proceedings of the 2005 {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {ACM},
	author = {Mandelin, David and Xu, Lin and Bodík, Rastislav and Kimelman, Doug},
	year = {2005},
	note = {event-place: Chicago, IL, USA},
	keywords = {program synthesis, mining, reuse},
	pages = {48--61},
	file = {ACM Full Text PDF:/home/vedran/Zotero/storage/4KQR68Z4/Mandelin et al. - 2005 - Jungloid Mining Helping to Navigate the API Jungl.pdf:application/pdf}
}

@article{kuncak_complete_2010,
	title = {Complete functional synthesis},
	volume = {45},
	issn = {0362-1340},
	url_comment = {http://dl.acm.org/citation.cfm?id=1806596.1806632},
	tmp_doi_comment = {10.1145/1806596.1806632},
	number = {6},
	urldate = {2019-03-22},
	journal = {ACM SIGPLAN Notices},
	author = {Kuncak, Viktor and Mayer, Mikaël and Piskac, Ruzica and Suter, Philippe and Kuncak, Viktor and Mayer, Mikaël and Piskac, Ruzica and Suter, Philippe},
	month = jun,
	year = {2010},
	pages = {316--329},
	file = {Snapshot:/home/vedran/Zotero/storage/AZANIXI3/citation.html:text/html;Submitted Version:/home/vedran/Zotero/storage/S2EVHMIT/Kuncak et al. - 2010 - Complete functional synthesis.pdf:application/pdf}
}

@article{bodik_algorithmic_2013,
	title = {Algorithmic program synthesis: introduction},
	volume = {15},
	issn = {1433-2787},
	shorttitle = {Algorithmic program synthesis},
	url_comment = {https://tmp_doi.org/10.1007/s10009-013-0287-9},
	tmp_doi_comment = {10.1007/s10009-013-0287-9},
	abstract = {Program synthesis is a process of producing an executable program from a specification. Algorithmic synthesis produces the program automatically, without an intervention from an expert. While classical compilation falls under the definition of algorithmic program synthesis, with the source program being the specification, the synthesis literature is typically concerned with producing programs that cannot be (easily) obtained with the deterministic transformations of a compiler. To this end, synthesis algorithms often perform a search, either in a space of candidate programs or in a space of transformations that might be composed to transform the specification into a desired program. In this introduction to the special journal issue, we survey the history of algorithmic program synthesis and introduce the contributed articles. We divide the field into reactive synthesis, which is concerned with automata-theoretic techniques for controllers that handle an infinite stream of requests, and functional synthesis, which produces programs consuming finite input. Contributed articles are divided analogously. We also provide pointers to synthesis work outside these categories and list many applications of synthesis.},
	language = {en},
	number = {5},
	urldate = {2018-09-12},
	journal = {International Journal on Software Tools for Technology Transfer},
	author = {Bodik, Rastislav and Jobstmann, Barbara},
	month = oct,
	year = {2013},
	keywords = {Controller Synthesis, Formal Methods, Program synthesis, Specifications of Program Correctness},
	pages = {397--411},
	file = {Springer Full Text PDF:/home/vedran/Zotero/storage/UJYS9J5Y/Bodik and Jobstmann - 2013 - Algorithmic program synthesis introduction.pdf:application/pdf}
}

@article{ison_tools_2016,
	title = {Tools and data services registry: a community effort to document bioinformatics resources},
	volume = {44},
	issn = {1362-4962},
	shorttitle = {Tools and data services registry},
	tmp_doi_comment = {10.1093/nar/gkv1116},
	abstract = {Life sciences are yielding huge data sets that underpin scientific discoveries fundamental to improvement in human health, agriculture and the environment. In support of these discoveries, a plethora of databases and tools are deployed, in technically complex and diverse implementations, across a spectrum of scientific disciplines. The corpus of documentation of these resources is fragmented across the Web, with much redundancy, and has lacked a common standard of information. The outcome is that scientists must often struggle to find, understand, compare and use the best resources for the task at hand.Here we present a community-driven curation effort, supported by ELIXIR-the European infrastructure for biological information-that aspires to a comprehensive and consistent registry of information about bioinformatics resources. The sustainable upkeep of this Tools and Data Services Registry is assured by a curation effort driven by and tailored to local needs, and shared amongst a network of engaged partners.As of November 2015, the registry includes 1785 resources, with depositions from 126 individual registrations including 52 institutional providers and 74 individuals. With community support, the registry can become a standard for dissemination of information about bioinformatics resources: we welcome everyone to join us in this common endeavour. The registry is freely available at https://bio.tools.},
	language = {eng},
	number = {D1},
	journal = {Nucleic Acids Research},
	author = {Ison, Jon and Rapacki, Kristoffer and Ménager, Hervé  and {others}},
	month = jan,
	year = {2016},
	pmid = {26538599},
	pmcid = {PMC4702812},
	keywords = {Computational Biology, Software, Data Curation, Registries, bio.tools},
	pages = {D38--47},
	annote = {biotoolsSchema},
	file = {Full Text:/home/vedran/Zotero/storage/JMCZ6HV8/Ison et al. - 2016 - Tools and data services registry a community effo.pdf:application/pdf}
}

@article{een_minisat_2005,
	title = {{MiniSat} : {A} {SAT} solver with conflict-clause minimization},
	shorttitle = {{MiniSat}},
	url_comment = {https://ci.nii.ac.jp/naid/10026031126/},
	urldate = {2018-09-23},
	journal = {Proc. SAT-05: 8th Int. Conf. on Theory and Applications of Satisfiability Testing},
	author = {EEN, N.},
	year = {2005},
	keywords = {miniSAT},
	pages = {502--518},
	file = {MiniSat \: A SAT solver with conflict-clause minimization Snapshot:/home/vedran/Zotero/storage/PASNARIQ/10026031126.html:text/html}
}
@misc{SWS_wiki,
	title = {Scientific workflow system},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Scientific_workflow_system},
	abstract = {A scientific workflow system is a specialized form of a workflow management system designed specifically to compose and execute a series of computational or data manipulation steps, or workflow, in a scientific application.},
	language = {en},
	urldate = {2019-02-15},
	journal = {Wikipedia},
	note = {[Online; 14 Feb 2019]}
}
@inproceedings{pnueli_synthesis_1989,
	address = {New York, NY, USA},
	series = {{POPL} '89},
	title = {On the {Synthesis} of a {Reactive} {Module}},
	isbn = {978-0-89791-294-5},
	url_comment = {http://tmp_doi.acm.org/10.1145/75277.75293},
	tmp_doi_comment = {10.1145/75277.75293},
	abstract = {We consider the synthesis of a reactive module with input x and output y, which is specified by the linear temporal formula @@@@(x, y). We show that there exists a program satisfying @@@@ iff the branching time formula (∀x) (∃y) A@@@@(x, y) is valid over all tree models. For the restricted case that all variables range over finite domains, the validity problem is decidable, and we present an algorithm for constructing the program whenever it exists. The algorithm is based on a new procedure for checking the emptiness of Rabin automata on infinite trees in time exponential in the number of pairs, but only polynomial in the number of states. This leads to a synthesis algorithm whose complexity is double exponential in the length of the given specification.},
	urldate = {2018-09-12},
	booktitle = {Proceedings of the 16th {ACM} {SIGPLAN}-{SIGACT} {Symposium} on {Principles} of {Programming} {Languages}},
	publisher = {ACM},
	author = {Pnueli, A. and Rosner, R.},
	year = {1989},
	pages = {179--190},
	annote = {This is the main paper where Pnueli presented importance of the Program Synthesis.},
	file = {ACM Full Text PDF:/home/vedran/Zotero/storage/EZ9IDBVE/Pnueli and Rosner - 1989 - On the Synthesis of a Reactive Module.pdf:application/pdf}
}

@incollection{antoniou_web_2004,
	address = {Berlin, Heidelberg},
	series = {International {Handbooks} on {Information} {Systems}},
	title = {Web {Ontology} {Language}: {OWL}},
	shorttitle = {Web {Ontology} {Language}},
	abstract = {SummaryIn order to extend the limited expressiveness of RDF Schema, a more expressive Web Ontology Language (OWL) has been defined by the World Wide Web Consortium (W3C). In this chapter we analyse the limitations of RDF Schema and derive requirements for a richer Web Ontology Language. We then describe the three-layered architecture of the OWL language, and we describe all of the language constructs of OWL in some detail. The chapter concludes with two extensive examples of OWL ontologies.},
	language = {en},
	urldate = {2019-02-26},
	booktitle = {Handbook on {Ontologies}},
	publisher = {Springer Berlin Heidelberg},
	author = {Antoniou, Grigoris and van Harmelen, Frank},
	editor = {Staab, Steffen and Studer, Rudi},
	year = {2004},
	keywords = {Description Logic, Datatype Property, Reasoning Support, Resource Description Framework, Resource Description Framework Schema},
	pages = {67--92}
}

@book{smith_pygmalion_1975,
	title = {Pygmalion: {A} {Creative} {Programming} {Environment}},
	shorttitle = {Pygmalion},
	abstract = {PYGMALION is a two-dimensional, visual programming system implemented on an interactive computer with graphics display. Communication between human being and computer is by means of visual entities called icons, subsuming the notions of variable, reference, data stucture, function and picture. The heart of the system is an interactive remembering editor for icons, which executes and (Optionally) saves operations for later reexecution. The display screen is viewed as a document to be edited. Programming consists of recreating a sequence of display frames, the last of which contains the desired information. Display frames are modified by editing operations.},
	language = {en},
	publisher = {Computer Science Department, Stanford University},
	author = {Smith, David Canfield},
	year = {1975},
	note = {Google-Books-ID: mihHAAAAIAAJ}
}

@incollection{summers_methodology_1986,
	title = {A {Methodology} for {LISP} {Program} {Construction} from {Examples}},
	isbn = {978-0-934613-12-5},
	abstract = {An automatic programming system, THESYS, for constructing recursive Lisp programs from examples of what they do is described. The construction methodology is illustrated as a series of transformations from the set of examples to a program satisfying the examples. The transformations consist of (1) deriving the specific computation associated with a specific example, (2) deriving control flow predicates, and (3) deriving an equivalent program specification in the form of recurrence relations. Equivalence between certain recurrence relations and various program schemata is proved. A detailed description of the construction of four programs is presented to illustrate the application of the methodology.},
	language = {en},
	booktitle = {Readings in {Artificial} {Intelligence} and {Software} {Engineering}},
	publisher = {Morgan Kaufmann},
	author = {Summers, PHILLIP D.},
	editor = {Rich, Charles and Waters, Richard C.},
	month = jan,
	year = {1986},
	keywords = {inductive inference, program synthesis, programming languages, recursive programs, sample computations},
	pages = {309--316},
	file = {ScienceDirect Snapshot:/home/vedran/Zotero/storage/89HXUH9A/B9780934613125500288.html:text/html}
}

@techreport{shaw_inferring_1975,
	title = {Inferring {LISP} {Programs} from {Examples}},
	institution = {Department of Computer Science, Columbia University},
	number = {CUCS-001-75},
	abstract = {A program is described which infers certain recursive LiSP programs from single example input-output parts. Synthesized programs may recur in more than one argument, and may involve the synthesis of auxilliary functions. An actual user session with the program, called EXAMPLE, is presented, and the operation of the program and its important heuristics are outlined.},
	language = {en},
	author = {Shaw, David Elliot and Swartout, William R. and Green, C. Cordell},
	year = {1975},
	file = {Full Text PDF:/home/vedran/Zotero/storage/FA384CED/Shaw et al. - 1975 - Inferring LISP Programs from Examples.pdf:application/pdf;Snapshot:/home/vedran/Zotero/storage/SJ57PZYJ/D89K4K6X.html:text/html}
}

@inproceedings{kautz_planning_1992,
	address = {Vienna, Austria},
	series = {{ECAI} '92},
	title = {Planning as satisfiability},
	isbn = {978-0-471-93608-4},
	urldate = {2020-02-01},
	booktitle = {Proceedings of the 10th {European} conference on {Artificial} intelligence},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Kautz, Henry and Selman, Bart},
	month = aug,
	year = {1992},
	pages = {359--363}
}


@inproceedings{kautz_pushing_1996,
	address = {Portland, Oregon},
	series = {{AAAI}'96},
	title = {Pushing the envelope: planning, propositional logic, and stochastic search},
	isbn = {978-0-262-51091-2},
	shorttitle = {Pushing the envelope},
	abstract = {Planning is a notoriously hard combinatorial search problem. In many interesting domains, current planning algorithms fail to scale up gracefully. By combining a general, stochastic search algorithm and appropriate problem encodings based on propositional logic, we are able to solve hard planning problems many times faster than the best current planning systems. Although stochastic methods have been shown to be very effective on a wide range of scheduling problems, this is the first demonstration of its power on truly challenging classical planning instances. This work also provides a new perspective on representational issues in planning.},
	urldate = {2020-02-01},
	booktitle = {Proceedings of the thirteenth national conference on {Artificial} intelligence - {Volume} 2},
	publisher = {AAAI Press},
	author = {Kautz, Henry and Selman, Bart},
	month = aug,
	year = {1996},
	pages = {1194--1201}
}


@article{di_tommaso_nextflow_2017,
	title = {Nextflow enables reproducible computational workflows},
	volume = {35},
	tmp_doi = {10.1038/nbt.3820},
	journal = {Nature Biotechnology},
	author = {Di Tommaso, Paolo and Chatzou, Maria and Floden, Evan W. and {others}},
	month = apr,
	year = {2017},
	pages = {316--319}
}

@article{koster_snakemakescalable_2012,
	title = {Snakemake—a scalable bioinformatics workflow engine},
	volume = {28},
	issn = {1367-4803},
	tmp_url = {https://academic.oup.com/bioinformatics/article/28/19/2520/290322},
	tmp_doi = {10.1093/bioinformatics/bts480},
	abstract = {Abstract.  Summary: Snakemake is a workflow engine that provides a readable Python-based workflow definition language and a powerful execution environment that},
	language = {en},
	number = {19},
	urldate = {2020-02-04},
	journal = {Bioinformatics},
	author = {Köster, Johannes and Rahmann, Sven},
	month = oct,
	year = {2012},
	pages = {2520--2522},
	file = {Full Text PDF:/home/vedran/Zotero/storage/N6VLMMIY/Köster and Rahmann - 2012 - Snakemake—a scalable bioinformatics workflow engin.pdf:application/pdf;Snapshot:/home/vedran/Zotero/storage/IJ6QVPEL/290322.html:text/html}
}

@article{amstutz_common_2016,
	title = {Common {Workflow} {Language}, v1.0},
	tmp_url = {https://www.research.manchester.ac.uk/portal/en/publications/common-workflow-language-v10(741919f5-d0ab-4557-9763-b811e911423b)/publications.html},
	tmp_doi = {10.6084/m9.figshare.3115156.v2},
	language = {English},
	urldate = {2020-02-04},
	author = {Amstutz, Peter and Crusoe, Michael R. and Tijanić, Nebojša and others},
	month = jul,
	year = {2016},
	file = {Full Text PDF:/home/vedran/Zotero/storage/DHU46ABD/Amstutz et al. - 2016 - Common Workflow Language, v1.0.pdf:application/pdf;Snapshot:/home/vedran/Zotero/storage/4LJAXUGI/publications.html:text/html}
}

@article{goecks_2010,
	title = {Galaxy: a comprehensive approach for supporting accessible, reproducible, and transparent computational research in the life sciences},
	volume = {11},
	issn = {1474-760X},
	shorttitle = {Galaxy},
	tmp_url = {https://tmp_doi.org/10.1186/gb-2010-11-8-r86},
	tmp_doi = {10.1186/gb-2010-11-8-r86},
	abstract = {Increased reliance on computational approaches in the life sciences has revealed grave concerns about how accessible and reproducible computation-reliant results truly are. Galaxy http://usegalaxy.org, an open web-based platform for genomic research, addresses these problems. Galaxy automatically tracks and manages data provenance and provides support for capturing the context and intent of computational methods. Galaxy Pages are interactive, web-based documents that provide users with a medium to communicate a complete computational analysis.},
	language = {en},
	number = {8},
	urldate = {2020-02-04},
	journal = {Genome Biology},
	author = {Goecks, Jeremy and Nekrutenko, Anton and Taylor, James and {others}},
	month = aug,
	year = {2010},
	keywords = {Analysis Workspace, Galaxy Server, Phylogenetic Profile, Public Repository, User Metadata},
	pages = {R86},
	file = {Springer Full Text PDF:/home/vedran/Zotero/storage/NC7ES8M6/Goecks et al. - 2010 - Galaxy a comprehensive approach for supporting ac.pdf:application/pdf}
}
@article{berthold2009knime,
  title={KNIME-the Konstanz information miner: version 2.0 and beyond},
  author={Berthold, Michael R and Cebron, Nicolas and Dill, Fabian and Gabriel, Thomas R and K{\"o}tter, Tobias and Meinl, Thorsten and Ohl, Peter and Thiel, Kilian and Wiswedel, Bernd},
  journal={AcM SIGKDD explorations Newsletter},
  volume={11},
  number={1},
  pages={26--31},
  year={2009},
  publisher={ACM New York, NY, USA}
}

@misc{toolsbiotoolsschema_2019,
	title = {bio-tools/{biotoolsSchema}},
	copyright = {CC-BY-SA-4.0},
	url = {https://github.com/bio-tools/biotoolsSchema},
	abstract = {biotoolsSchema : Tool description data model for computational tools in life sciences},
	urldate = {2020-02-04},
	publisher = {bio.tools},
	month = dec,
	year = {2019},
	note = {original-date: 2015-05-05T15:52:46Z}
}
@inproceedings{lawrence1997workflow,
  title={Workflow handbook},
  author={Lawrence, Peter and Bouzeghoub, Mokrane and Fabret, Fran{\c{c}}oise and Matulovic-broqu{\'e}, Maja},
  booktitle={In Proc. Intl. Workshop on Design and Management of Data Warehouses (DMDW’99},
  year={1997},
  organization={Citeseer}
}

@book{fischer2002workflow,
  title={Workflow handbook 2002},
  author={Fischer, Layna},
  year={2002},
  publisher={Future Strategies Inc.}
}

@article{van2005yawl,
  title={YAWL: yet another workflow language},
  author={Van Der Aalst, Wil MP and Ter Hofstede, Arthur HM},
  journal={Information systems},
  volume={30},
  number={4},
  pages={245--275},
  year={2005},
  publisher={Elsevier}
}

@article{amstutz2016common,
  title={Common workflow language, v1. 0},
  author={Amstutz, Peter and Crusoe, Michael R and Tijani{\'c}, Neboj{\v{s}}a and Chapman, Brad and Chilton, John and Heuer, Michael and Kartashov, Andrey and Leehr, Dan and M{\'e}nager, Herv{\'e} and Nedeljkovich, Maya and others},
  year={2016},
  publisher={Figshare}
}


@article{vivian_toil_2017,
	title = {Toil enables reproducible, open source, big biomedical data analyses},
	volume = {35},
	issn = {1546-1696},
	url = {http://www.nature.com/articles/nbt.3772},
	doi = {10.1038/nbt.3772},
	language = {en},
	number = {4},
	urldate = {2020-04-15},
	journal = {Nature Biotechnology},
	author = {Vivian, John and Rao, Arjun Arkal and Nothaft, Frank Austin and Ketchum, Christopher and Armstrong, Joel and Novak, Adam and Pfeil, Jacob and Narkizian, Jake and Deran, Alden D. and Musselman-Brown, Audrey and Schmidt, Hannes and Amstutz, Peter and Craft, Brian and Goldman, Mary and Rosenbloom, Kate and Cline, Melissa and O'Connor, Brian and Hanna, Megan and Birger, Chet and Kent, W. James and Patterson, David A. and Joseph, Anthony D. and Zhu, Jingchun and Zaranek, Sasha and Getz, Gad and Haussler, David and Paten, Benedict},
	month = apr,
	year = {2017},
	pages = {314--316},
	file = {Full Text PDF:/home/vedran/Zotero/storage/QZPUW2D2/Vivian et al. - 2017 - Toil enables reproducible, open source, big biomed.pdf:application/pdf;Snapshot:/home/vedran/Zotero/storage/ZGXSNXYC/nbt.html:text/html}
}
@misc{noauthor_workflow_2020,
	title = {Workflow {Description} {Language} ({WDL})},
	copyright = {BSD-3-Clause},
	url = {https://github.com/openwdl/wdl},
	abstract = {Workflow Description Language},
	urldate = {2020-04-15},
	publisher = {openwdl},
	month = apr,
	year = {2020},
	note = {original-date: 2012-08-01T03:12:48Z},
	keywords = {bioinformatics, cloud, cromwell, openwdl, reproducibility, reproducible-science, wdl, workflow}
}
@InProceedings{kasalica2020,
author="Kasalica, Vedran
and Lamprecht, Anna-Lena",
editor="Krzhizhanovskaya, Valeria V.
and Z{\'a}vodszky, G{\'a}bor
and Lees, Michael H.
and Dongarra, Jack J.
and Sloot, Peter M. A.
and Brissos, S{\'e}rgio
and Teixeira, Jo{\~a}o",
title="APE: A Command-Line Tool and API for Automated Workflow Composition",
booktitle="Computational Science -- ICCS 2020",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="464--476",
abstract="Automated workflow composition is bound to take the work with scientific workflows to the next level. On top of today's comprehensive eScience infrastructure, it enables the automated generation of possible workflows for a given specification. However, functionality for automated workflow composition tends to be integrated with one of the many available workflow management systems, and is thus difficult or impossible to apply in other environments. Therefore we have developed APE (the Automated Pipeline Explorer) as a command-line tool and API for automated composition of scientific workflows. APE is easily configured to a new application domain by providing it with a domain ontology and semantically annotated tools. It can then be used to synthesize purpose-specific workflows based on a specification of the available workflow inputs, desired outputs and possibly additional constraints. The workflows can further be transformed into executable implementations and/or exported into standard workflow formats. In this paper we describe APE v1.0 and discuss lessons learned from applications in bioinformatics and geosciences.",
isbn="978-3-030-50436-6"
}

@article{georgakopoulos_overview_1995,
	title = {An overview of workflow management: {From} process modeling to workflow automation infrastructure},
	volume = {3},
	issn = {1573-7578},
	shorttitle = {An overview of workflow management},
	url = {https://doi.org/10.1007/BF01277643},
	doi = {10.1007/BF01277643},
	abstract = {Today's business enterprises must deal with global competition, reduce the cost of doing business, and rapidly develop new services and products. To address these requirements enterprises must constantly reconsider and optimize the way they do business and change their information systems and applications to support evolving business processes. Workflow technology facilitates these by providing methodologies and software to support (i) business process modeling to capture business processes as workflow specifications, (ii) business process reengineering to optimize specified processes, and (iii) workflow automation to generate workflow implementations from workflow specifications. This paper provides a high-level overview of the current workflow management methodologies and software products. In addition, we discuss the infrastructure technologies that can address the limitations of current commercial workflow technology and extend the scope and mission of workflow management systems to support increased workflow automation in complex real-world environments involving heterogeneous, autonomous, and distributed information systems. In particular, we discuss how distributed object management and customized transaction management can support further advances in the commercial state of the art in this area.},
	language = {en},
	number = {2},
	urldate = {2020-07-07},
	journal = {Distributed and Parallel Databases},
	author = {Georgakopoulos, Diimitrios and Hornick, Mark and Sheth, Amit},
	month = apr,
	year = {1995},
	pages = {119--153}
}

@Book{Lampre2013,
  Title                    = {{U}ser-{L}evel {W}orkflow {D}esign - {A} {B}ioinformatics {P}erspective},
  Author                   = {Anna-Lena Lamprecht},
  Publisher                = {Springer},
  Year                     = {2013},
  Series                   = {Lecture Notes in Computer Science},
  Volume                   = {8311},

  Doi                      = {10.1007/978-3-642-45389-2},
  Ee                       = {http://dx.doi.org/10.1007/978-3-642-45389-2},
  File                     = {Lampre2012 (User-Level Workflow Design - A Bioinformatics Perspective).pdf:Lampre2012 (User-Level Workflow Design - A Bioinformatics Perspective).pdf:PDF},
  ISBN                     = {978-3-642-45388-5, 978-3-642-45389-2},
  Keywords                 = {bio-jeti, prophets, bioinformatics, synthesis, ser-level workflow design},
  Owner                    = {Lamprecht},
  Pages                    = {1-202},
  School                   = {Technische Universität Dortmund},
  Timestamp                = {2012.07.04},
  Type                     = {Dissertation}
}

@InCollection{KaMaRT2010,
  Title                    = {{W}orkflow {C}omposition and {E}nactment {U}sing j{ORCA}},
  Author                   = {Karlsson, Johan and Martín-Requena, Victoria and Ríos, Javier and {others}},
  Booktitle                = {Leveraging Applications of Formal Methods, Verification, and Validation},
  Publisher                = {Springer Berlin / Heidelberg},
  Year                     = {2010},
  Pages                    = {328-339},
  Series                   = {Lecture Notes in Computer Science},
  Volume                   = {6415},

  Affiliation              = {Computer Architecture Department, University of Malaga, Campus de Teatinos, 29071 Málaga, Spain},
  File                     = {KaMaRT2010 (Workflow Composition and Enactment Using jORCA).pdf:KaMaRT2010 (Workflow Composition and Enactment Using jORCA).pdf:PDF},
  Keywords                 = {bioinformatics, workflows, synthesis},
  Owner                    = {Lamprecht},
  Timestamp                = {2011.07.04},
  Tmp_Url                      = {http://dx.doi.org/10.1007/978-3-642-16558-0_28}
}

@InProceedings{LaNaMS2010,
  Title                    = {{S}ynthesis-{B}ased {L}oose {P}rogramming},
  Author                   = {Anna-Lena Lamprecht and Stefan Naujokat and Tiziana Margaria and Bernhard Steffen},
  Booktitle                = {QUATIC 2010, Porto, Portugal},
  Year                     = {2010},
  Month                    = sep,
  Pages                    = {262-267},
  Publisher                = {IEEE},

  Abstract                 = {In this paper we present loose programming, an approach designed to enable process developers to design their application-specific processes in an intuitive style. Key to this approach is the concept of loose specification, a graphical formalism that allows developers to express their processes just by sketching them as kinds of flow graphs without caring about types, precise knowledge about the available process components or the availability of resources. They only have to specify the rough process flow graphically in terms of ontologically defined ‘semantic’ entities. These loose specifications are then concretized to fully executable process code automatically by means of a combination of 1) data-flow analysis, ensuring the availability of the required resources, 2) temporal logic-based process synthesis, resolving type conflicts and taking care of correct component instantiation, and 3) model checking, to ensure global intents and invariants expressed in temporal logic.},
  Tmp_Doi                      = {10.1109/QUATIC.2010.53},
  File                     = {LaNaMS2010 (Synthesis-Based Loose Programming).pdf:LaNaMS2010 (Synthesis-Based Loose Programming).pdf:PDF},
  Keywords                 = {synthesis, loose programming, loose specification},
  Owner                    = {naujokat}
}

@TechReport{StMaFr1993,
  Title                    = {{M}odule {C}onfiguration by {M}inimal {M}odel {C}onstruction},
  Author                   = {Bernhard Steffen and Tiziana Margaria and Burkhard Freitag},
  Institution              = {Fakultät für Mathematik und Informatik, Universität Passau},
  Year                     = {1993},

  File                     = {StMaFr1993 (Module Configuration by Minimal Model Construction).pdf:StMaFr1993 (Module Configuration by Minimal Model Construction).pdf:PDF},
  Keywords                 = {synthesis},
  Owner                    = {naujokat},
  Timestamp                = {2009.11.23}
}


@article{martin_high-level_2009,
	title = {High-{Level} {Synthesis}: {Past}, {Present}, and {Future}},
	volume = {26},
	issn = {0740-7475},
	doi = {10.1109/MDT.2009.83},
	number = {4},
	journal = {IEEE Design Test of Computers},
	author = {Martin, G. and Smith, G.},
	month = jul,
	year = {2009},
	keywords = {History, design and test, high level synthesis, High level synthesis, high-level synthesis, history, Algorithm design and analysis, behavioral synthesis, commercial use, Commercialization, Computer industry, Electronic design automation and methodology, Electronics industry, ESL synthesis, Hardware, High-level synthesis, HLS tools, industry adoption, Machinery production industries, Physics computing, system-level design},
	pages = {18--25}
}


@misc{cwl_computational_ws,
	title = {Computational {Data} {Analysis} {Workflow} {Systems}},
	url = {https://github.com/common-workflow-language/common-workflow-language},
	abstract = {Repository for the CWL standards. Use https://cwl.discourse.group/ for support 😊 - common-workflow-language/common-workflow-language},
	language = {en},
	urldate = {2020-07-16},
	journal = {GitHub},
	note = {Library Catalog: github.com},
	file = {Snapshot:/home/vedran/Zotero/storage/SPWL2FE4/Existing-Workflow-systems.html:text/html}
}

